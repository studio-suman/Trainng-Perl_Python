{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import PyPDF2\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = ''\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() #type: ignore\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of PDF file paths containing resumes\n",
    "pdf_files = ['resume1.pdf', 'resume2.pdf', 'resume3.pdf', 'resume4.pdf', 'resume5.pdf'] # replace resumes with filepaths to resumes for the analysis\n",
    "\n",
    "# Extract text from each PDF resume and store it in a list\n",
    "resumes_text = [extract_text_from_pdf(pdf_path) for pdf_path in pdf_files]\n",
    "\n",
    "# Create a DataFrame with columns 'ID' and 'resume_text'\n",
    "data = pd.DataFrame({'ID': range(1, len(pdf_files)+1), 'resume_text': resumes_text})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "data.to_csv('resumes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from CSV file\n",
    "data = pd.read_csv('resumes.csv')\n",
    "\n",
    "# Load spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "\n",
    "# Add entity ruler pipeline to spaCy model\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", before=\"ner\")\n",
    "\n",
    "# Define patterns as dictionaries\n",
    "patterns = [\n",
    "    {\"label\": \"SKILL\", \"pattern\": [{\"LOWER\": \"skill_1\"}]},\n",
    "    {\"label\": \"SKILL\", \"pattern\": [{\"LOWER\": \"skill_2\"}]},\n",
    "    {\"label\": \"SKILL\", \"pattern\": [{\"LOWER\": \"skill_3\"}]},\n",
    "    {\"label\": \"SKILL\", \"pattern\": [{\"LOWER\": \"skill_4\"}]}\n",
    "]  # \"LOWER\" ensures that variations in case (uppercase, lowercase, title case) are all matched by the same pattern.\n",
    "\n",
    "# Add patterns to entity ruler\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('punkt')  # Download the 'punkt' tokenizer resource\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Initialize WordNet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove hyperlinks, special characters, and punctuations using regex\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    text = re.sub(r'[^\\w\\s\\n]', '', text)\n",
    "\n",
    "    # Convert the text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Tokenize the text using nltk's word_tokenize\n",
    "    words = word_tokenize(text)\n",
    "\n",
    "    # Lemmatize the text to its base form for normalization\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    # Remove English stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = ' '.join([word for word in lemmatized_words if word not in stop_words])\n",
    "\n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean the 'resume_text' column in the DataFrame\n",
    "data['cleaned_resume'] = data['resume_text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "# Define options for visualization\n",
    "options = {'ents': ['PERSON', 'GPE', 'SKILL'],\n",
    "           'colors': {'PERSON': 'orange',\n",
    "                      'GPE': 'lightgreen',\n",
    "                      'SKILL': 'lightblue'}}\n",
    "\n",
    "# Visualize named entities in each resume\n",
    "for resume_text in data['cleaned_resume']:\n",
    "    doc = nlp(resume_text)\n",
    "    displacy.render(doc, style=\"ent\", jupyter=True, options=options)\n",
    "    print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Define the company requirements\n",
    "company_requirements = \"Company Requirements\"\n",
    "\n",
    "# Combine the company requirements with stopwords removed\n",
    "cleaned_company_requirements = clean_text(company_requirements)\n",
    "\n",
    "# Calculate TF-IDF vectors for the company requirements and resume texts\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['cleaned_resume'])\n",
    "company_tfidf = tfidf_vectorizer.transform([cleaned_company_requirements])\n",
    "\n",
    "# Calculate cosine similarity between the company requirements and each resume\n",
    "similarity_scores = cosine_similarity(company_tfidf, tfidf_matrix).flatten()\n",
    "\n",
    "# Get the indices of resumes sorted by similarity score\n",
    "sorted_indices = similarity_scores.argsort()[::-1]\n",
    "\n",
    "# Display the top 5 most similar resumes\n",
    "top_n = 5\n",
    "for i in range(top_n):\n",
    "    index = sorted_indices[i]\n",
    "    print(f\"Resume ID: {data['ID'][index]}\")\n",
    "    print(f\"Similarity Score: {similarity_scores[index]}\")\n",
    "    print(data['resume_text'][index])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(resume_text, required_skills):\n",
    "    # Process the resume text with the spaCy model\n",
    "    doc = nlp(resume_text)\n",
    "\n",
    "    # Extract skills from the resume using the entity ruler\n",
    "    skills = [ent.text.lower() for ent in doc.ents if ent.label_ == \"SKILL\"]\n",
    "\n",
    "    # Calculate the number of matching skills with required skills\n",
    "    matching_skills = [skill for skill in skills if skill in required_skills]\n",
    "    num_matching_skills = len(matching_skills)\n",
    "\n",
    "    # Calculate the similarity score\n",
    "    similarity_score = num_matching_skills / max(len(required_skills), len(skills))\n",
    "\n",
    "    return similarity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in data[['cleaned_resume']].itertuples(index = False):\n",
    "  resume_text = str(text[0])\n",
    "  print(resume_text)\n",
    "  required_skills = [\"skill_1\", \"skill_2\", \"skill_3\", \"skill_4\"]\n",
    "  similarity_score = calculate_similarity(resume_text, required_skills)\n",
    "  print(\"Similarity Score:\", similarity_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
