{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Model Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installed LangChain \n",
    "#%pip install langchain langchain-community faiss-cpu langchain-chroma langchain-openai langchain_text_splitters langgraph beautifulsoup4 langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader # type: ignore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings # type: ignore\n",
    "from langchain_community.vectorstores import FAISS # type: ignore\n",
    "from langchain.chains import RetrievalQA # type: ignore\n",
    "from langchain_ollama.llms import OllamaLLM # type: ignore\n",
    "from langchain_ollama import OllamaEmbeddings # type: ignore\n",
    "from langchain_core.messages import HumanMessage, SystemMessage # type: ignore\n",
    "from langchain_core.prompts import ChatPromptTemplate # type: ignore\n",
    "\n",
    "from langchain.cache import InMemoryCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"ce53c5fce80c4503927244333e40634c\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://wdc-chat-llm.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4o\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2024-06-01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment = os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    api_version = os.environ[\"OPENAI_API_VERSION\"],\n",
    "    temperature= 0.3\n",
    ")\n",
    "langchain.llm_cache = InMemoryCache() # type: ignore\n",
    "parser = StrOutputParser()\n",
    "#embeddings = HuggingFaceEmbeddings() #model=\"sentence-transformers/all-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Bengali\"),\n",
    "    HumanMessage(content=\"hi!\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"What is current date and time in India?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have real-time capabilities to provide the current date and time. However, you can easily find the current date and time in India by checking a reliable world clock website, using a smartphone, or asking a digital assistant like Siri or Google Assistant. India is in the Indian Standard Time (IST) zone, which is UTC+5:30.\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm.invoke(prompt)\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'হ্যালো!'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm.invoke(messages)\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm | parser\n",
    "chain.invoke({\"language\": \"bengali\", \"text\": \"how are you doing\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To calculate the average distance of Earth from the Sun, we'll consider a few key factors:\\n\\n1. **Astronomical Unit (AU)**: The AU is a standard unit used to measure distances in our solar system. It's equal to the average distance between the Earth and the Sun.\\n\\n2. **Orbit of the Earth**: The Earth orbits the Sun at an average speed of about 29.78 kilometers per second (km/s). This speed, combined with its orbital period of approximately 365.25 days, allows us to calculate its average distance from the Sun.\\n\\nNow, let's use these values to find the answer:\\n\\nAverage distance (in AU) = Average speed / Orbital period\\n\\nFirst, we need to convert the speed from km/s to AU per day. There are 24 hours in a day and approximately 365.25 days in a year.\\n\\nSpeed in AU/day ≈ 29.78 km/s × (1 AU / 149,597,890 km) × (1 day / 24 hours) ≈ 0.0000167 AU/day\\n\\nNow we can calculate the average distance:\\n\\nAverage distance = Average speed / Orbital period\\n= 0.0000167 AU/day ÷ 365.25 days\\n≈ 0.00437 AU\\n\\nSince 1 AU is approximately equal to 93 million miles, we can convert this value to miles by multiplying it with 93 million.\\n\\nAverage distance ≈ 0.00437 AU × 93,000,000 miles/AU ≈ 410,800 kilometers or approximately 255,000 miles.\\n\\nSo, the average distance of Earth from the Sun is about **255,000 miles** (or 405,500 kilometers).\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%pip install langchain_ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(\n",
    "    model=\"llama3.2\",\n",
    "    top_k=3\n",
    "    )\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"How far Earth from Sun?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain faiss-cpu unstructured unstructured[pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath: rag_implementation.py\n",
    "\n",
    "\n",
    "# Load documents from a local folder\n",
    "def load_documents_from_folder(folder_path):\n",
    "    #loader = DirectoryLoader(folder_path,glob=\"**/*.pdf\",show_progress=True)\n",
    "    loader = PyPDFDirectoryLoader(folder_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "# Embed and vectorize documents using FAISS\n",
    "def embed_and_vectorize_documents(documents):\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    return vector_store\n",
    "\n",
    "# Implement RAG\n",
    "def implement_rag(vector_store, query):\n",
    "    llm  = OllamaLLM(\n",
    "    model=\"llama3.2\"\n",
    "    )\n",
    "    retriever = vector_store.as_retriever()\n",
    "    rag_chain = RetrievalQA(llm=llm, retriever=retriever)\n",
    "    result = rag_chain.run(query)\n",
    "    return result\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"As Technical Recruiter Evaluate the following resumes\"),\n",
    "    HumanMessage(content=\"Hello Recruiter\"),\n",
    "]\n",
    "\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "\n",
    "\"\"\"prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ") \"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = r\"C:\\Users\\HSASS\\OneDrive - Wipro\\Desktop\\Temp2\\Resume\"\n",
    "    query = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")])\n",
    "\n",
    "    # Load documents\n",
    "    documents = load_documents_from_folder(folder_path)\n",
    "\n",
    "    # Embed and vectorize documents\n",
    "    vector_store = embed_and_vectorize_documents(documents)\n",
    "\n",
    "    # Implement RAG and get the result\n",
    "    result = implement_rag(vector_store, query)\n",
    "    print(result)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
