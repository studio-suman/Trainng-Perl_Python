{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Model Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installed LangChain \n",
    "#%pip install langchain langchain-community faiss-cpu langchain-chroma langchain-openai langchain_text_splitters langgraph beautifulsoup4 langchain-huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install langchain_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import datetime\n",
    "import os\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader # type: ignore\n",
    "from langchain.embeddings import HuggingFaceEmbeddings # type: ignore\n",
    "from langchain_community.vectorstores import FAISS # type: ignore\n",
    "from langchain.chains import RetrievalQA # type: ignore\n",
    "from langchain_ollama.llms import OllamaLLM # type: ignore\n",
    "from langchain_ollama import OllamaEmbeddings # type: ignore\n",
    "from langchain_core.messages import HumanMessage, SystemMessage # type: ignore\n",
    "from langchain_core.prompts import ChatPromptTemplate # type: ignore\n",
    "\n",
    "from langchain.cache import InMemoryCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"ce53c5fce80c4503927244333e40634c\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://wdc-chat-llm.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4o\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2024-06-01\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment = os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    api_version = os.environ[\"OPENAI_API_VERSION\"],\n",
    "    temperature= 0.3\n",
    ")\n",
    "langchain.llm_cache = InMemoryCache() # type: ignore\n",
    "parser = StrOutputParser()\n",
    "#embeddings = HuggingFaceEmbeddings() #model=\"sentence-transformers/all-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English into Bengali\"),\n",
    "    HumanMessage(content=\"hi!\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"How are you doing?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"Translate the following into {language}:\"\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(messages)\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt_template | llm | parser\n",
    "chain.invoke({\"language\": \"bengali\", \"text\": \"how are you doing\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To calculate the average distance of Earth from the Sun, we need to consider a few factors.\\n\\nStep 1: The average distance between the Earth and the Sun is called an astronomical unit (AU). One AU is equal to the average distance between the Earth and the Sun, which is approximately 93 million miles or 149.6 million kilometers.\\n\\nStep 2: To get more precise measurements, we can use the elliptical shape of the Earth's orbit around the Sun. At its closest point (perihelion), the distance is about 91.4 million miles or 147 million kilometers. At its farthest point (aphelion), the distance is about 94.5 million miles or 152.1 million kilometers.\\n\\nStep 3: The Earth's orbit is not a perfect circle, so we need to use an average value that takes into account both perihelion and aphelion distances. A commonly used average distance is 93.8 million miles (151.4 million kilometers) from the Sun.\\n\\nSo, to summarize, the average distance of Earth from the Sun is approximately 93.8 million miles or 151.4 million kilometers.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%pip install langchain_ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "template = \"\"\"Question: {question}\n",
    "\n",
    "Answer: Let's think step by step.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "model = OllamaLLM(\n",
    "    model=\"llama3.2\",\n",
    "    top_k=3\n",
    "    )\n",
    "\n",
    "chain = prompt | model\n",
    "\n",
    "chain.invoke({\"question\": \"How far Earth from Sun?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install langchain faiss-cpu unstructured[pdf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath: rag_implementation.py\n",
    "\n",
    "\n",
    "# Load documents from a local folder\n",
    "def load_documents_from_folder(folder_path):\n",
    "    #loader = DirectoryLoader(folder_path,glob=\"**/*.pdf\",show_progress=True)\n",
    "    loader = PyPDFDirectoryLoader(folder_path)\n",
    "    documents = loader.load()\n",
    "    return documents\n",
    "\n",
    "# Embed and vectorize documents using FAISS\n",
    "def embed_and_vectorize_documents(documents):\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    return vector_store\n",
    "\n",
    "# Implement RAG\n",
    "def implement_rag(vector_store, query):\n",
    "    llm  = OllamaLLM(\n",
    "    model=\"llama3.2\"\n",
    "    )\n",
    "    retriever = vector_store.as_retriever()\n",
    "    rag_chain = RetrievalQA(llm=llm, retriever=retriever)\n",
    "    result = rag_chain.run(query)\n",
    "    return result\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"As Technical Recruiter Evaluate the following resumes\"),\n",
    "    HumanMessage(content=\"Hello Recruiter\"),\n",
    "]\n",
    "\n",
    "system_template = \"Translate the following into {language}:\"\n",
    "\n",
    "\"\"\"prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
    ") \"\"\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = r\"C:\\Users\\HSASS\\OneDrive - Wipro\\Desktop\\Temp2\\Resume\"\n",
    "    query = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", \"{text}\")])\n",
    "\n",
    "    # Load documents\n",
    "    documents = load_documents_from_folder(folder_path)\n",
    "\n",
    "    # Embed and vectorize documents\n",
    "    vector_store = embed_and_vectorize_documents(documents)\n",
    "\n",
    "    # Implement RAG and get the result\n",
    "    result = implement_rag(vector_store, query)\n",
    "    print(result)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
