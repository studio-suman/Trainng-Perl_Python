{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5ec0e75-fbc0-4d17-91fd-5b57ef31ad9f",
   "metadata": {},
   "source": [
    "# Document Completion API Workflow\n",
    "\n",
    "The Document Completion API enables users to dynamically extract relevant information from documents using a process based on Retrieval-Augmented Generation (RAG). This involves leveraging language models alongside information retrieval techniques to generate accurate and contextual responses.\n",
    "\n",
    "The process consists of four main steps:\n",
    "1. Create Dataset\n",
    "2. Ingest Dataset\n",
    "3. Prepare Dataset\n",
    "4. Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9cd355-7632-4acf-b911-1fe9af00a1ee",
   "metadata": {},
   "source": [
    "## Create Dataset\n",
    "\n",
    "   This endpoint is used to create a new dataset in the system. A dataset serves as a logical grouping to store and organize    all documents uploaded by the user. It is the foundation for subsequent steps, such as document ingestion and querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "274f9638-b553-40b9-8c33-d50d104cfd54",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'd90b8b53-da18-4b85-8fb7-6c23f67fb2ed', 'desc': 'Some test Dataset1', 'files': [], 'name': 'Test Dataset1', 'owners': ['edee4f9f-873f-439e-983c-8f732d312177'], 'tenant_id': 'a919164d-8b7c-43fb-8119-f1997d45ca4f'}\n"
     ]
    }
   ],
   "source": [
    "import requests  # Import the requests module to send HTTP requests\n",
    "\n",
    "# Define the API endpoint for the datasets\n",
    "datasets_endpoint = f\"https://api.lab45.ai/v1.1/datasets\"\n",
    "\n",
    "# Set the headers for the request, including the content type, accepted response format and authorization token\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Accept\": \"text/event-stream, application/json\",\n",
    "    \"Authorization\": \"Bearer <api_key>\" #Replace <api_key> with your API key for authentication\n",
    "}\n",
    "\n",
    "# Payload data to be sent with the request, typically in JSON format. This payload defines a new dataset.\n",
    "payload = {\n",
    "            \"name\": \"Test Dataset1\",\n",
    "            \"description\": \"Some test Dataset1\"\n",
    "            }\n",
    "\n",
    "# Make the POST request to the datasets API endpoint with the provided headers and payload\n",
    "response = requests.post(datasets_endpoint, headers=headers, json=payload)\n",
    "\n",
    "# Print the response json from the API call (this contains the name, id and the other field values related to this api request)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3928d010-ab6c-4aa1-916e-73324636f33c",
   "metadata": {},
   "source": [
    "## Ingest Dataset\n",
    "   \n",
    "   The Ingest Dataset endpoint is used to upload document data into a previously created dataset. The uploaded documents are    stored in a blob storage for future processing and retrieval. This step ensures that the dataset is populated with the       necessary documents, enabling the system to perform document completion tasks efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e0ea5b5-410a-42ed-b8e1-360d3c6779f6",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_id\":\"d90b8b53-da18-4b85-8fb7-6c23f67fb2ed\",\"desc\":\"Some test Dataset1\",\"files\":[{\"name\":\"test_data.txt\",\"ts\":1737615558.063004},{\"name\":\"test_data2.txt\",\"ts\":1737615558.0630112}],\"name\":\"Test Dataset1\",\"owners\":[\"edee4f9f-873f-439e-983c-8f732d312177\"],\"tenant_id\":\"a919164d-8b7c-43fb-8119-f1997d45ca4f\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Define the API endpoint for the ingest process, replacing `{dataset_id}` with the dataset ID\n",
    "url = \"https://api.lab45.ai/v1.1/datasets/d90b8b53-da18-4b85-8fb7-6c23f67fb2ed/ingest\"\n",
    "\n",
    "payload = {}\n",
    "\n",
    "files = [\n",
    "    ('test_sample',('test_data.txt',open('./test_data.txt','rb'),'text/plain')),\n",
    "    ('test_sample2',('test_data2.txt',open('./test_data2.txt','rb'),'text/plain'))\n",
    "]\n",
    "headers = {\n",
    "  \"Authorization\": \"Bearer <api_key>\" #Replace <api_key> with your API key for authentication\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload, files=files)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83bfb75-5c07-43c1-8d7d-a1751a5b12bc",
   "metadata": {},
   "source": [
    "## Prepare Dataset\n",
    "   \n",
    "   The Prepare Dataset step is critical in the document completion workflow. It converts the ingested documents into            embeddings, which are dense vector representations of the document content. These embeddings encode the semantic             meaning of the text, allowing the system to retrieve relevant information efficiently based on user queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c6a5fd53-0fbf-4b9b-880d-a109f9ebb31e",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"_id\":\"54ccfbc7-358a-4e99-84b9-19b8b12b89f1\",\"emb_type\":\"openai\",\"resource_group_id\":\"d90b8b53-da18-4b85-8fb7-6c23f67fb2ed\",\"status\":\"Started\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "prepare_endpoint = f\"https://api.lab45.ai/v1.1/skills/doc_completion/prepare\"\n",
    "\n",
    "# Set the headers for the request, including the content type, accepted response format, and authorization token\n",
    "headers = {\n",
    "    'Content-Type': \"application/json\",\n",
    "    'Accept': \"text/event-stream, application/json\",\n",
    "    \"Authorization\": \"Bearer <api_key>\" #Replace <api_key> with your API key for authentication\n",
    "}\n",
    "\n",
    "# Define the payload (request body) for the API call, which includes a dataset ID\n",
    "payload ={\n",
    "    \"dataset_id\": \"d90b8b53-da18-4b85-8fb7-6c23f67fb2ed\"\n",
    "}\n",
    "\n",
    "# Make the POST request to the \"prepare\" API endpoint with the provided headers and payload\n",
    "response = requests.post(prepare_endpoint, headers=headers, json=payload)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c943076-5f58-4730-a282-84ce2245bef9",
   "metadata": {},
   "source": [
    "## Query\n",
    "\n",
    "   The Query step enables users to ask questions or make requests, leveraging the processed dataset for context-aware           responses. The system uses embeddings and indexed document content to identify relevant sections of the dataset that         match the query.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a5fd97b-dfb5-4e0d-9a3b-a24f27ab2f14",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"data\": {\"content\": \"The document provided consists of two main parts. The first part describes a platform that is an AI-powered digital assistant offering comprehensive and reliable information across various topics. It emphasizes the platform's ability to provide accurate and detailed responses for educational support, general knowledge insights, writing assistance, and guidance on different subjects. The platform is committed to delivering complete and trustworthy information while maintaining a safe and respectful environment for all users.\\n\\nThe second part of the document provides information about cricket, a popular bat-and-ball sport played between\"}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://api.lab45.ai/v1.1/skills/doc_completion/query\"\n",
    "\n",
    "payload = json.dumps({\n",
    "    \"dataset_id\" : \"d90b8b53-da18-4b85-8fb7-6c23f67fb2ed\",  \n",
    "    \"skill_parameters\": {  \n",
    "        \"model_name\": \"gpt-4o\", \n",
    "        \"retrieval_chain\": \"custom\",  \n",
    "        \"emb_type\": \"openai\",  \n",
    "        \"temperature\": 0,  \n",
    "        \"max_output_tokens\": 100,  \n",
    "        \"return_sources\": False  \n",
    "    },\n",
    "    \"stream_response\": False, \n",
    "    \"messages\": [  \n",
    "        {\"content\": \"Hi\", \"role\": \"user\"},  \n",
    "        {\"content\": \"give summary of the uploaded document\", \"role\": \"user\"} \n",
    "    ]\n",
    "})\n",
    "headers = {\n",
    "  'Content-Type': 'application/json',\n",
    "  \"Authorization\": \"Bearer <api_key>\" #Replace <api_key> with your API key for authentication\n",
    "}\n",
    "\n",
    "response = requests.request(\"POST\", url, headers=headers, data=payload)\n",
    "\n",
    "print(response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
