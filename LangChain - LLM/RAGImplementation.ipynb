{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA, StuffDocumentsChain\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_openai import AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"ce53c5fce80c4503927244333e40634c\"\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://wdc-chat-llm.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4o\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2024-06-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents from a local folder\n",
    "def load_documents_from_folder(folder_path):\n",
    "    #loader = DirectoryLoader(folder_path,glob=\"**/*.pdf\",show_progress=True)\n",
    "    loader = PyPDFDirectoryLoader(folder_path)\n",
    "    documents = loader.load()\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed and vectorize documents using FAISS\n",
    "def embed_and_vectorize_documents(documents):\n",
    "    embeddings = OllamaEmbeddings(model=\"llama3.2\")\n",
    "    vector_store = FAISS.from_documents(documents, embeddings)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement RAG\n",
    "def implement_rag(vector_store, query):\n",
    "    \"\"\"     llm  = OllamaLLM(\n",
    "    model=\"llama3.2\",\n",
    "    top_k=3\n",
    "    ) \"\"\"\n",
    "    \n",
    "    llm = AzureChatOpenAI(\n",
    "    azure_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment = os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    api_version = os.environ[\"OPENAI_API_VERSION\"],\n",
    "    temperature= 0.3\n",
    "    )\n",
    "    \n",
    "    retriever = vector_store.as_retriever()\n",
    "    prompt = PromptTemplate(template=\"Answer the question based on the context: {context}\\n\\nQuestion: {question}\\nAnswer:\",input_variables=[\"context\", \"question\"])\n",
    "    combine_documents_chain = StuffDocumentsChain(llm_chain=llm)\n",
    "    rag_chain = RetrievalQA(retriever=retriever, combine_documents_chain=combine_documents_chain)\n",
    "    result = rag_chain.run(query)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if __name__ == \"__main__\":\n",
    "def run_vector_AI():\n",
    "    folder_path = r\"D:\\OneDrive - Wipro\\Desktop\\Temp2\\Resume\\\\\"\n",
    "\n",
    "    #query=PromptTemplate(format=\"\")\n",
    "    # Load documents\n",
    "    documents = load_documents_from_folder(folder_path)\n",
    "    if not documents:\n",
    "        print(\"No documents found in the specified folder.\")\n",
    "    else:\n",
    "        print(f\"Loaded {len(documents)} documents.\")\n",
    "\n",
    "    # Embed and vectorize documents\n",
    "    vector_store = embed_and_vectorize_documents(documents)\n",
    "    print(\"Vector Store created.\")\n",
    "    return vector_store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 26 documents.\n"
     ]
    }
   ],
   "source": [
    "# Define the query\n",
    "query = \"What is the summary of the documents?\"\n",
    "vector_store = run_vector_AI()\n",
    "# Implement RAG and get the result\n",
    "result = implement_rag(vector_store,query)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
