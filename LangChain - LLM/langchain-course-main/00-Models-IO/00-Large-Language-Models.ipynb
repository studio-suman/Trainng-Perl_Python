{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcded1a6-2430-4707-a78c-c82f4c5ee6fc",
   "metadata": {},
   "source": [
    "<a href = \"https://www.pieriantraining.com\"><img src=\"../PT Centered Purple.png\"> </a>\n",
    "\n",
    "<em style=\"text-align:center\">Copyrighted by Pierian Training</em>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728f1747-b8fc-4d31-96c2-047fc83c079d",
   "metadata": {},
   "source": [
    "# Language Models\n",
    "\n",
    "**Note: For other Non-OpenAI models, you can check out: https://python.langchain.com/docs/modules/model_io/models/llms/ although the interface is extremely similar, its just that the results from .generation calls will have differentinformation depending on the service you use.**\n",
    "\n",
    "## Text Model Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f49cecee-a933-4f48-b1e1-dca183fd07a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588bdc1a-b02e-44c0-be67-893db4c75b07",
   "metadata": {},
   "source": [
    "You can stored your API key however you prefer, its common to set it as an environment variable, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dee0df5e-2ceb-4071-ae19-b3d446f79490",
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ['OpenAI_API_KEY'] = 'ce53c5fce80c4503927244333e40634c'\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = \"https://wdc-chat-llm.openai.azure.com/\"\n",
    "os.environ[\"AZURE_OPENAI_API_KEY\"] = \"ce53c5fce80c4503927244333e40634c\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2024-06-01\"\n",
    "os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"] = \"gpt-4o\"\n",
    "SecretStr = os.environ[\"AZURE_OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c53a9e5-8637-4815-a834-caa8ce7bbfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce53c5fce80c4503927244333e40634c\n",
      "ce53c5fce80c4503927244333e40634c\n"
     ]
    }
   ],
   "source": [
    "print(os.environ['OPenAI_API_KEY'])\n",
    "print(os.environ[\"AZURE_OPENAI_API_KEY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e96b83-d6d1-4867-9416-1742aeeb8ef8",
   "metadata": {},
   "source": [
    "Note, that LangChain automatically looks up for any environment variable with the name OPENAI_API_KEY automatically when making a connection to OpenAI. Alternatively, you could just pass in the openai key via a string (not very secure, but okay for your own local projects), or even just save it somewhere on your computer in a text file and then read it in, for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f71ca3e5-05dd-40e9-a2d3-26e186a365af",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('C:\\\\Users\\\\Marcial\\\\Desktop\\\\desktop_openai.txt')\n",
    "api_key = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28549f54-2645-4824-b5dd-06e3b063341d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm = OpenAI(openai_api_key=api_key)\n",
    "#llm = AzureOpenAI(model=\"gpt -3.5-turbo-instruct\")\n",
    "llm = AzureChatOpenAI(\n",
    "    azure_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment = os.environ[\"AZURE_OPENAI_DEPLOYMENT_NAME\"],\n",
    "    api_version = os.environ[\"OPENAI_API_VERSION\"],\n",
    "    temperature= 0.9\n",
    ")\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e8ada-ae30-40a7-84ea-64cd08887315",
   "metadata": {},
   "source": [
    "## Text Model Call\n",
    "\n",
    "This is the simplest way to get a text autocomplete:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93f7d94f-5683-48be-b852-86eb8ac90ef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Pluto, once considered the ninth planet of our solar system, was reclassified as a \"dwarf planet\" by the International Astronomical Union (IAU) in 2006. It is located in the Kuiper Belt, a region of the solar system beyond Neptune that is filled with small icy bodies. Despite its reclassification, many still hold a sentimental attachment to Pluto as the \"ninth planet.\" One fascinating fact about Pluto is that it has five known moons: Charon, which is the largest and nearly half the size of Pluto itself, and four smaller moons named Nix, Hydra, Kerberos, and Styx.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm.invoke('Here is a fun fact about Pluto:')\n",
    "parser.invoke(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78677192-2c35-43fb-89f2-8d5ce9d7e30e",
   "metadata": {},
   "source": [
    "You can also use generate to get full output with more info:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802ffd32-a6bc-4319-875c-ea1dffc5e324",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python311\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:369: UserWarning: Unexpected type for token usage: <class 'NoneType'>\n",
      "  warnings.warn(f\"Unexpected type for token usage: {type(new_usage)}\")\n"
     ]
    }
   ],
   "source": [
    "# NEEDS TO BE A LIST, EVEN FOR JUST ONE STRING\n",
    "result = llm.generate(['Here is a fun fact about Pluto:',\n",
    "                       'Here is a fun fact about Mars:']\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf0c3c6a-eab9-4b6c-a785-63f799cc23a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'$defs': {'BaseMessage': {'additionalProperties': True,\n",
       "   'description': 'Base abstract message class.\\n\\nMessages are the inputs and outputs of ChatModels.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type', 'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'type'],\n",
       "   'title': 'BaseMessage',\n",
       "   'type': 'object'},\n",
       "  'BaseMessageChunk': {'additionalProperties': True,\n",
       "   'description': 'Message chunk, which can be concatenated with other Message chunks.',\n",
       "   'properties': {'content': {'anyOf': [{'type': 'string'},\n",
       "      {'items': {'anyOf': [{'type': 'string'}, {'type': 'object'}]},\n",
       "       'type': 'array'}],\n",
       "     'title': 'Content'},\n",
       "    'additional_kwargs': {'title': 'Additional Kwargs', 'type': 'object'},\n",
       "    'response_metadata': {'title': 'Response Metadata', 'type': 'object'},\n",
       "    'type': {'title': 'Type', 'type': 'string'},\n",
       "    'name': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Name'},\n",
       "    'id': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Id'}},\n",
       "   'required': ['content', 'type'],\n",
       "   'title': 'BaseMessageChunk',\n",
       "   'type': 'object'},\n",
       "  'ChatGeneration': {'description': 'A single chat generation output.\\n\\nA subclass of Generation that represents the response from a chat model\\nthat generates chat messages.\\n\\nThe `message` attribute is a structured representation of the chat message.\\nMost of the time, the message will be of type `AIMessage`.\\n\\nUsers working with chat models will usually access information via either\\n`AIMessage` (returned from runnable interfaces) or `LLMResult` (available\\nvia callbacks).',\n",
       "   'properties': {'text': {'default': '', 'title': 'Text', 'type': 'string'},\n",
       "    'generation_info': {'anyOf': [{'type': 'object'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Generation Info'},\n",
       "    'type': {'const': 'ChatGeneration',\n",
       "     'default': 'ChatGeneration',\n",
       "     'enum': ['ChatGeneration'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'message': {'$ref': '#/$defs/BaseMessage'}},\n",
       "   'required': ['message'],\n",
       "   'title': 'ChatGeneration',\n",
       "   'type': 'object'},\n",
       "  'ChatGenerationChunk': {'description': 'ChatGeneration chunk, which can be concatenated with other\\nChatGeneration chunks.',\n",
       "   'properties': {'text': {'default': '', 'title': 'Text', 'type': 'string'},\n",
       "    'generation_info': {'anyOf': [{'type': 'object'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Generation Info'},\n",
       "    'type': {'const': 'ChatGenerationChunk',\n",
       "     'default': 'ChatGenerationChunk',\n",
       "     'enum': ['ChatGenerationChunk'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'},\n",
       "    'message': {'$ref': '#/$defs/BaseMessageChunk'}},\n",
       "   'required': ['message'],\n",
       "   'title': 'ChatGenerationChunk',\n",
       "   'type': 'object'},\n",
       "  'Generation': {'description': 'A single text generation output.\\n\\nGeneration represents the response from an \"old-fashioned\" LLM that\\ngenerates regular text (not chat messages).\\n\\nThis model is used internally by chat model and will eventually\\nbe mapped to a more general `LLMResult` object, and then projected into\\nan `AIMessage` object.\\n\\nLangChain users working with chat models will usually access information via\\n`AIMessage` (returned from runnable interfaces) or `LLMResult` (available\\nvia callbacks). Please refer the `AIMessage` and `LLMResult` schema documentation\\nfor more information.',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'generation_info': {'anyOf': [{'type': 'object'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Generation Info'},\n",
       "    'type': {'const': 'Generation',\n",
       "     'default': 'Generation',\n",
       "     'enum': ['Generation'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['text'],\n",
       "   'title': 'Generation',\n",
       "   'type': 'object'},\n",
       "  'GenerationChunk': {'description': 'Generation chunk, which can be concatenated with other Generation chunks.',\n",
       "   'properties': {'text': {'title': 'Text', 'type': 'string'},\n",
       "    'generation_info': {'anyOf': [{'type': 'object'}, {'type': 'null'}],\n",
       "     'default': None,\n",
       "     'title': 'Generation Info'},\n",
       "    'type': {'const': 'Generation',\n",
       "     'default': 'Generation',\n",
       "     'enum': ['Generation'],\n",
       "     'title': 'Type',\n",
       "     'type': 'string'}},\n",
       "   'required': ['text'],\n",
       "   'title': 'GenerationChunk',\n",
       "   'type': 'object'},\n",
       "  'RunInfo': {'description': 'Class that contains metadata for a single execution of a Chain or model.\\n\\nDefined for backwards compatibility with older versions of langchain_core.\\n\\nThis model will likely be deprecated in the future.\\n\\nUsers can acquire the run_id information from callbacks or via run_id\\ninformation present in the astream_event API (depending on the use case).',\n",
       "   'properties': {'run_id': {'format': 'uuid',\n",
       "     'title': 'Run Id',\n",
       "     'type': 'string'}},\n",
       "   'required': ['run_id'],\n",
       "   'title': 'RunInfo',\n",
       "   'type': 'object'}},\n",
       " 'description': 'A container for results of an LLM call.\\n\\nBoth chat models and LLMs generate an LLMResult object. This object contains\\nthe generated outputs and any additional information that the model provider\\nwants to return.',\n",
       " 'properties': {'generations': {'items': {'items': {'anyOf': [{'$ref': '#/$defs/Generation'},\n",
       "      {'$ref': '#/$defs/ChatGeneration'},\n",
       "      {'$ref': '#/$defs/GenerationChunk'},\n",
       "      {'$ref': '#/$defs/ChatGenerationChunk'}]},\n",
       "    'type': 'array'},\n",
       "   'title': 'Generations',\n",
       "   'type': 'array'},\n",
       "  'llm_output': {'anyOf': [{'type': 'object'}, {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'title': 'Llm Output'},\n",
       "  'run': {'anyOf': [{'items': {'$ref': '#/$defs/RunInfo'}, 'type': 'array'},\n",
       "    {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'title': 'Run'},\n",
       "  'type': {'const': 'LLMResult',\n",
       "   'default': 'LLMResult',\n",
       "   'enum': ['LLMResult'],\n",
       "   'title': 'Type',\n",
       "   'type': 'string'}},\n",
       " 'required': ['generations'],\n",
       " 'title': 'LLMResult',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ff190d3a-a99c-4079-8a9c-3ef150efc75d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'completion_tokens': 178,\n",
       "  'prompt_tokens': 30,\n",
       "  'total_tokens': 208,\n",
       "  'completion_tokens_details': None,\n",
       "  'prompt_tokens_details': None},\n",
       " 'model_name': None,\n",
       " 'system_fingerprint': 'fp_67802d9a6d'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0e7d94-7452-4907-a0e1-06347146075f",
   "metadata": {},
   "source": [
    "# Chat Models\n",
    "\n",
    "The most popular models are actually chat models, that have a System Message and then a series of Assistant and Human Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c4f384c-be31-4e5f-8a67-e390e479489c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb524f31-3d3a-4a4b-bc00-d21843490193",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c6555a0-3bfb-49e3-97aa-186eb5a528a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat([HumanMessage(content=\"Can you tell me a fact about Earth?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58180fce-e04b-41d2-9ae8-87a41b232890",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='One fact about Earth is that it is the only known planet to support life.', additional_kwargs={}, example=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "375586e2-7880-49c5-abf3-dbdf70593b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One fact about Earth is that it is the only known planet to support life.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be3fb5bc-13e3-45de-98a9-45fe1361d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat([SystemMessage(content='You are a very rude teenager who only wants to party and not answer questions'),\n",
    "               HumanMessage(content='Can you tell me a fact about Earth?')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "013daa46-53e4-47dc-83c9-5037c8286feb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ugh, seriously? Fine, here's a fact for you. Earth is the third planet from the Sun in our solar system. Now, can we move on to something more exciting? Like, I don't know, partying or something?\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c34f9d0c-3a14-4595-a1fc-96f25c5b206c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEEDS TO BE A LIST!\n",
    "result = chat.generate(\n",
    "                [\n",
    "                [SystemMessage(content='You are a University Professor'),\n",
    "               HumanMessage(content='Can you tell me a fact about Earth?')]\n",
    "                ]\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d8c5f0ec-3989-461b-abd3-d1494cdc9b3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='Certainly! One interesting fact about Earth is that it is the only known planet in our solar system where water exists in all three states: solid (ice), liquid (water), and gas (water vapor). This unique combination of water in different states allows for the existence of life as we know it on Earth.', generation_info=None, message=AIMessage(content='Certainly! One interesting fact about Earth is that it is the only known planet in our solar system where water exists in all three states: solid (ice), liquid (water), and gas (water vapor). This unique combination of water in different states allows for the existence of life as we know it on Earth.', additional_kwargs={}, example=False))]], llm_output={'token_usage': {'prompt_tokens': 25, 'completion_tokens': 62, 'total_tokens': 87}, 'model_name': 'gpt-3.5-turbo'}, run=[RunInfo(run_id=UUID('075174db-b350-4076-8b34-f43ea59169dd'))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11cbbb7c-060f-4aa5-88a5-a1b4693430da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token_usage': {'prompt_tokens': 25,\n",
       "  'completion_tokens': 62,\n",
       "  'total_tokens': 87},\n",
       " 'model_name': 'gpt-3.5-turbo'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "031dcac3-b2e7-40d3-8e18-c3185fc085b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Certainly! One interesting fact about Earth is that it is the only known planet in our solar system where water exists in all three states: solid (ice), liquid (water), and gas (water vapor). This unique combination of water in different states allows for the existence of life as we know it on Earth.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.generations[0][0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5026430-caaa-4f9c-938e-328b2f383c5b",
   "metadata": {},
   "source": [
    "## Extra Parameters and Args\n",
    "\n",
    "Here we add in some extra parameters and args, note we chose some pretty extreme values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe01c99b-b14f-4358-a532-765a19bb5666",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = chat([HumanMessage(content='Can you tell me a fact about Earth?')],\n",
    "                 temperature=2,presence_penalty=1,max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "91a67f85-9c18-4ac3-9b2e-ff021c121a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cycling remains one of the mosPopular physical activities performed on Earth'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5964b698-ba1b-4c2f-a23a-5e757dd84e2a",
   "metadata": {},
   "source": [
    "# Caching\n",
    "\n",
    "Making the same exact request often? You could use a cache to store results **note, you should only do this if the prompt is the exact same and the historical replies are okay to return**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a3341f0b-6524-4711-a019-ab9ba497e4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import langchain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(openai_api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1639f253-5b37-4ffc-b028-c22b3df2b877",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One interesting fact about Mars is that it has the largest volcano in the solar system. Named Olympus Mons, this shield volcano stands about 13.6 miles (22 kilometers) high and spans approximately 370 miles (600 kilometers) in diameter. It is nearly three times the height of Mount Everest, making it the tallest volcano and one of the largest known volcanoes in the entire solar system.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.cache import InMemoryCache\n",
    "langchain.llm_cache = InMemoryCache()\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "llm.predict(\"Tell me a fact about Mars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0d706fd5-6067-4d7e-80ab-cd96ccd4a912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One interesting fact about Mars is that it has the largest volcano in the solar system. Named Olympus Mons, this shield volcano stands about 13.6 miles (22 kilometers) high and spans approximately 370 miles (600 kilometers) in diameter. It is nearly three times the height of Mount Everest, making it the tallest volcano and one of the largest known volcanoes in the entire solar system.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You will notice this reply is instant!\n",
    "llm.predict('Tell me a fact about Mars')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ee731f-1933-4260-a16a-63a2dccbbacc",
   "metadata": {},
   "source": [
    "You can also use SQLite Caches: https://python.langchain.com/docs/modules/model_io/models/chat/how_to/chat_model_caching#sqlite-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e061d-c254-4e64-b81a-5dbd4f0cc59a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
